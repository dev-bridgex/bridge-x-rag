{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kEcDInFvNAr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da28f65d-12c3-4e9b-9d67-65e88583a739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  35964      0 --:--:-- --:--:-- --:--:-- 35991\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_model_id_1 = \"gemma2:9b-instruct-q5_0\"\n",
        "ollama_model_id_2 =\"nomic-embed-text\"\n",
        "\n",
        "#!pkill -f ollama"
      ],
      "metadata": {
        "id": "9bmz88Xwkt_O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama serve\" &\n",
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8001 OLLAMA_ORIGIN=* ollama serve\" &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "id": "WzKxAim5kz5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee9d273-f339-46bd-cde1-cb941b9e973b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n",
            "time=2025-04-23T01:41:23.941Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-04-23T01:41:23.941Z level=INFO source=routes.go:1298 msg=\"Listening on [::]:8000 (version 0.6.5)\"\n",
            "time=2025-04-23T01:41:23.941Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "2025/04/23 01:41:24 routes.go:1231: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:8001 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-04-23T01:41:24.151Z level=INFO source=images.go:458 msg=\"total blobs: 0\"\n",
            "time=2025-04-23T01:41:24.151Z level=INFO source=images.go:465 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-04-23T01:41:24.151Z level=INFO source=routes.go:1298 msg=\"Listening on [::]:8001 (version 0.6.5)\"\n",
            "time=2025-04-23T01:41:24.151Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-04-23T01:41:24.462Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-3278533e-e629-253f-f694-6344fd6081f7 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.5 GiB\"\n",
            "time=2025-04-23T01:41:24.463Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-3278533e-e629-253f-f694-6344fd6081f7 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.5 GiB\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull {ollama_model_id_1}\n",
        "!ollama pull {ollama_model_id_2}"
      ],
      "metadata": {
        "id": "5cYUWSdflw6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5648d103-576c-4022-ae85-dc6347577bb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n",
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8000 OLLAMA_ORIGIN=* ollama run {ollama_model_id_1}\"  &\n",
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:8001 OLLAMA_ORIGIN=* ollama run {ollama_model_id_2}\"  &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "id": "Bmk6k5ZamNv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52441517-ec93-4831-b572-7df3a228de83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n",
            "pulling a04ed3642d33...   7% ▕█               ▏ 451 MB/6.5 GB  149 MB/s     40s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 233 MB/274 MB   77 MB/s      0s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling a04ed3642d33...   7% ▕█               ▏ 452 MB/6.5 GB  149 MB/s     40s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 233 MB/274 MB   77 MB/s      0s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling a04ed3642d33...   7% ▕█               ▏ 452 MB/6.5 GB  113 MB/s     53s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 234 MB/274 MB   77 MB/s      0s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling a04ed3642d33...   7% ▕█               ▏ 452 MB/6.5 GB  113 MB/s     53s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 234 MB/274 MB   77 MB/s      0s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling a04ed3642d33...   7% ▕█               ▏ 475 MB/6.5 GB  113 MB/s     53s\u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 240 MB/274 MB   77 MB/s      0s\u001b[K\u001b[?25h\u001b[?2026l"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl http://localhost:8000/api/chat -d '{\n",
        "  \"model\": \"gemma2:9b-instruct-q5_0\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"ما عاصمة مصر؟\" }\n",
        "  ]\n",
        "}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5uaHnX9nYtn",
        "outputId": "e1e39d28-980a-42b0-d839-fe483e601a0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"error\":\"model \\\"gemma2:9b-instruct-q5_0\\\" not found, try pulling it first\"}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   219  100    77  100   142   9649  17794 --:--:-- --:--:-- --:--:-- 31285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl http://localhost:8001/api/embeddings -d '{\n",
        "  \"model\": \"nomic-embed-text\",\n",
        "  \"prompt\": \"The sky is blue because of Rayleigh scattering\"\n",
        "}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF0krTmTo-YT",
        "outputId": "38a39925-e7e7-4909-dc73-cf37c1155fc7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"error\":\"model \\\"nomic-embed-text\\\" not found, try pulling it first\"}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   165  100    70  100    95  11581  15718 --:--:-- --:--:-- --:--:-- 33000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ngrok**"
      ],
      "metadata": {
        "id": "Z1zDTz_kocK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyngrok==7.2.0"
      ],
      "metadata": {
        "id": "25UEuqCj2_7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "ngrok_auth = userdata.get('colab-ngrok')\n",
        "\n",
        "conf.get_default().auth_token = ngrok_auth\n",
        "\n",
        "port = \"8000\"\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "7-ed7sLMoe2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "ngrok_auth = userdata.get('colab-ngrok')\n",
        "\n",
        "conf.get_default().auth_token = ngrok_auth\n",
        "\n",
        "port = \"8001\"\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "KdJKhdZaqA84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl https://2905-34-125-18-107.ngrok-free.app/api/chat -d '{\n",
        "  \"model\": \"gemma2:9b-instruct-q5_0\",\n",
        "  \"stream\": false,\n",
        "  \"messages\": [\n",
        "    { \"role\": \"user\", \"content\": \"ما عاصمة مصر؟\" }\n",
        "  ]\n",
        "}'"
      ],
      "metadata": {
        "id": "6KTyx1vwydg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://5a4b-34-125-18-107.ngrok-free.app/api/embeddings\"\n",
        "data = {\n",
        "    \"model\": \"nomic-embed-text\",\n",
        "    \"prompt\": \"تعرف ايه عن القاهره\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "response.raise_for_status()\n",
        "\n",
        "embedding = response.json().get(\"embedding\")\n",
        "if embedding:\n",
        "    print(\"number of Dimensions\", len(embedding))\n",
        "else:\n",
        "    print(\"الـ embedding Error API.\")\n"
      ],
      "metadata": {
        "id": "GZoD7oPjvOut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}